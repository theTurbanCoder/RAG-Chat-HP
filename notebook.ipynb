{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3f74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e29dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Structure \n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_community.document_loaders import  PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from pathlib import PosixPath\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c51e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_sort_key(path: PosixPath) -> int:\n",
    "    \"\"\"\n",
    "    Custom sorting key to extract the numerical part of the filename.\n",
    "    \n",
    "    It finds the first sequence of digits in the path's string representation\n",
    "    and converts it to an integer for accurate sorting.\n",
    "    \"\"\"\n",
    "    # Convert the PosixPath to a string\n",
    "    path_str = str(path)\n",
    "    \n",
    "    # Use regex to find one or more digits (\\d+)\n",
    "    match = re.search(r'\\d+', path_str)\n",
    "    \n",
    "    if match:\n",
    "        # Convert the found digits (e.g., \"5\") into an integer (5)\n",
    "        return int(match.group(0))\n",
    "    else:\n",
    "        # Return a fallback value (e.g., 0) if no number is found, \n",
    "        # though it's unlikely with your current file names.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34a86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the files from a pdf file in a directory \n",
    "\n",
    "def process_all_pdfs(directoryName):\n",
    "\n",
    "    all_docs = []\n",
    "    pdf_dir = Path(directoryName)\n",
    "\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    pdf_files = sorted(pdf_files, key=numerical_sort_key)\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for file in pdf_files:\n",
    "        print(f\"Processing file {file.name} \")\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "                doc.metadata['book_title'] = \"book_\"+file.name.split(\".\")[0]\n",
    "            \n",
    "            all_docs.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "    \n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c02594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 PDF files to process\n",
      "Processing file 1.pdf \n",
      "Loaded 118 pages\n",
      "Processing file 2.pdf \n",
      "Loaded 359 pages\n",
      "Processing file 3.pdf \n",
      "Loaded 455 pages\n",
      "Processing file 4.pdf \n",
      "Loaded 243 pages\n",
      "Processing file 5.pdf \n",
      "Loaded 891 pages\n",
      "Processing file 6.pdf \n",
      "Loaded 671 pages\n",
      "Processing file 7.pdf \n",
      "Loaded 638 pages\n"
     ]
    }
   ],
   "source": [
    "all_docs  = process_all_pdfs(directoryName=\"./knowledge_base\")\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2871774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splittign and Chunking \n",
    "\n",
    "def split_documents(documents, chunk_size=300, chunk_overlap=50):\n",
    "\n",
    "    \"\"\" Split documents to smaller chunks for better RAG performance\"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"])\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd49e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13bbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3bb6b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'ProcessText Group',\n",
       " 'creator': '',\n",
       " 'creationdate': 'D:20080417232732',\n",
       " 'author': '',\n",
       " 'title': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'source': 'knowledge_base/1.pdf',\n",
       " 'total_pages': 118,\n",
       " 'page': 60,\n",
       " 'page_label': '61',\n",
       " 'source_file': '1.pdf',\n",
       " 'file_type': 'pdf',\n",
       " 'book_title': 'book_1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1000].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abcf5025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "\n",
    "    def __init__(self, model_name:str = \"all-MiniLM-L6-v2\"):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            print(f\"Loading Embedded Model: {self.model_name}\")\n",
    "            \n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "            print(f\"Model Loaded SuccessFully. Embedding Dimenison: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error Printing model {self.model_name}: {e}\")\n",
    "            pass\n",
    "\n",
    "    def generate_embeddings(self, texts:List[str]):\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model Not Loaded\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a707631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedded Model: all-MiniLM-L6-v2\n",
      "Model Loaded SuccessFully. Embedding Dimenison: 384\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f995fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "\n",
    "    def __init__(self, collection_name:str=\"harry_potter_clean_v1\", persist_directory:str = \"./knowledge_base/vector_store\"):\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.initialize_store()\n",
    "\n",
    "    def initialize_store(self):\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection =  self.client.get_or_create_collection(name=self.collection_name, metadata={\"hnsw:space\": \"cosine\"}) # <--- Add this line)\n",
    "            print(f\"VectorStore Initialized : {self.collection_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Vectore Store: {e}\")\n",
    "            raise ValueError(\"Vector Store Not Created\")\n",
    "\n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "\n",
    "        for i, (doc, embedding ) in enumerate (zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        try:\n",
    "            # self.collection.add(ids=ids, embeddings=embeddings_list, metadatas=metadatas, documents=documents_text)\n",
    "            # print(f\"Successfully added {len(documents)} documents to the vector store\")\n",
    "            # print(f\"Total documents in collection : {self.collection.count()}\")\n",
    "            # pass\n",
    "            batches = create_batches(\n",
    "                api=self.collection._client,\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            \n",
    "            )\n",
    "\n",
    "            for batch_ids, batch_embeddings, batch_metadatas, batch_documents in batches:\n",
    "                self.collection.add(\n",
    "                    ids=batch_ids,\n",
    "                    embeddings=batch_embeddings,\n",
    "                    metadatas=batch_metadatas,\n",
    "                    documents=batch_documents\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Couldnt add data to the Vector Store {e}\")\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c87813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7acbe9845434d3484716b771c1fabf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings_texts = embeddings.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f34b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore Initialized : harry_potter_clean_v1\n"
     ]
    }
   ],
   "source": [
    "vector_store = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522c04bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0078942 , -0.00971607,  0.05580289, ...,  0.084111  ,\n",
       "        -0.07735459, -0.04348418],\n",
       "       [-0.06369741, -0.01781824, -0.00157029, ...,  0.02127322,\n",
       "         0.05857925, -0.0687658 ],\n",
       "       [-0.03480296,  0.01225663, -0.01224808, ...,  0.06081391,\n",
       "         0.08534583, -0.06727676],\n",
       "       ...,\n",
       "       [-0.04582788,  0.02903817,  0.03912584, ...,  0.04433487,\n",
       "        -0.01902489,  0.03317221],\n",
       "       [-0.00095898,  0.02377967,  0.01469692, ...,  0.02154875,\n",
       "        -0.09594901,  0.0410526 ],\n",
       "       [-0.05036232,  0.1451934 ,  0.03554253, ...,  0.0233979 ,\n",
       "        -0.00728701,  0.0656971 ]], shape=(26592, 384), dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154e4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(chunks,embeddings_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12c0e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "\n",
    "    def __init__(self, vector_store:VectorStore,embedding_manager:Embeddings):\n",
    "\n",
    "        self.vector_store:VectorStore = vector_store\n",
    "        self.embedding_manager:Embeddings = embedding_manager\n",
    "        self.reranker_model = CrossEncoder('BAAI/bge-reranker-large', max_length=512)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def retrive(self,query: str, top_k:int = 10, score_threshold:float = 0.0, filter: Dict[str,str]=None) -> List[Dict[str, Any]]:\n",
    "        \n",
    "\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        try:\n",
    "\n",
    "            results = self.vector_store.collection.query(query_embeddings=[query_embedding.tolist()], \n",
    "                            n_results=top_k)\n",
    "\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents =  results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids=results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata,distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "\n",
    "                    similarity_score = 1 - distance \n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id':doc_id,'content':document, 'metadata':metadata, 'similarity_score':similarity_score, 'distance':distance, 'rank':i+1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No Documents found\")\n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"No documents found\")\n",
    "            return []\n",
    "            pass\n",
    "\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # 2. Add 'self' and proper type hints\n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Reranks retrieved documents using a cross-encoder model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare sentence pairs for cross-encoder: [[query, doc1_content], [query, doc2_content], ...]\n",
    "        sentence_pairs = [[query, doc['content']] for doc in documents]\n",
    "        \n",
    "        # Get relevance scores using the initialized model\n",
    "        with torch.no_grad():\n",
    "            scores = self.reranker_model.predict(sentence_pairs)\n",
    "    \n",
    "        # CRITICAL FIX: Save the score into the document dictionary\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc['rerank_score'] = float(scores[i])\n",
    "\n",
    "        # Sort by the new 'rerank_score' key\n",
    "        ranked_docs = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        return ranked_docs[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "784ae056",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(vector_store=vector_store, embedding_manager=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2eb59f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7303245867d7413cb8922a898297e1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n"
     ]
    }
   ],
   "source": [
    "rd= rag_retriever.retrive(\"dumbledore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d0411ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.rerank(\"dumbledore\",rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee6959c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"gemma:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392c7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(model='gemma:7b', messages=[{\"role\":\"system\", \"content\":\"hi i am tanveeer and you are ?\"}], max_tokens=200, n=1, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6623a744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-765', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a large language model, trained by Google. I am here to assist you with information, answer your questions, and engage in meaningful conversations.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1763620323, model='gemma:7b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=31, prompt_tokens=33, total_tokens=64, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f24d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_response(content_string):\n",
    "    # This defines the structure: response.choices[0].message.content\n",
    "    MockMessage = type('MockMessage', (object,), {'content': content_string})\n",
    "    MockChoice = type('MockChoice', (object,), {'message': MockMessage()})\n",
    "    MockResponse = type('MockResponse', (object,), {'choices': [MockChoice()]})\n",
    "    return MockResponse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f092d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_simple(query, retriever, llm, top_k=5, use_rerank=True):\n",
    "    print(f\"ðŸ”Ž Searching for: {query}\")\n",
    "    \n",
    "    # 1. RETRIEVE: Get 2x candidates to give the Reranker options to choose from\n",
    "    initial_results = retriever.retrive(query, top_k=top_k * 2)\n",
    "    \n",
    "    if not initial_results:\n",
    "        return \"I couldn't find any information on that in the books.\"\n",
    "\n",
    "    # 2. RERANK: Filter down to the best top_k\n",
    "    if use_rerank:\n",
    "        print(\"ðŸ”„ Reranking results...\")\n",
    "        final_results = retriever.rerank(query, initial_results, top_k=top_k)\n",
    "    else:\n",
    "        final_results = initial_results[:top_k]\n",
    "\n",
    "    # 3. GENERATE\n",
    "    context = \"\\n\\n\".join([f\"[Extract]: {doc['content']}\" for doc in final_results])\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert on the Harry Potter universe. Answer strictly based on the provided book excerpts.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    You are an expert on the Harry Potter books.\n",
    "    Answer the user's question with a concise, single paragraph.\n",
    "    \n",
    "    RULES:\n",
    "    1. Base your answer **STRICTLY AND ONLY** on the context provided below.\n",
    "    2. If the context does not contain the answer, state: 'The provided excerpts do not contain the answer to this question.'\n",
    "    \n",
    "    CONTEXT:\n",
    "    ---\n",
    "    {context}\n",
    "    ---\n",
    "    \n",
    "    QUESTION: {query}\n",
    "    \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # 3. TEMPORARILY RETURN THE RAW CONTEXT (Comment out the real generation step)\n",
    "    # debug_text = f\"--- DEBUG: RETRIEVED CONTEXT ---\\n{context}\\n\\n--- END CONTEXT ---\"\n",
    "    # return create_mock_response(debug_text)\n",
    "\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        model='gemma:7b', \n",
    "        messages=messages, \n",
    "        max_tokens=500, \n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83f5f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_character_profile(character_name, retriever, llm):\n",
    "    query = f\"Who is {character_name}? What is their appearance, personality, and key role?\"\n",
    "    \n",
    "    # Get broad context\n",
    "    results = retriever.retrive(query, top_k=15) # High k to catch mentions across books\n",
    "    if retriever.reranker_model:\n",
    "        results = retriever.rerank(query, results, top_k=7)\n",
    "        \n",
    "    context = \"\\n\".join([doc['content'] for doc in results])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a magical historian compiling files for the Ministry of Magic.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Create a dossier for: {character_name} using ONLY the provided text.\n",
    "        \n",
    "        Format the output exactly like this:\n",
    "        **Name:** {character_name}\n",
    "        **Affiliation:** (e.g. Hogwarts House, Death Eater, Order of the Phoenix)\n",
    "        **Physical Description:** (Hair, eyes, distinct features)\n",
    "        **Key Personality Traits:**\n",
    "        **Notable Magical Feats/Events:**\n",
    "        \n",
    "        CONTEXT:\n",
    "        {context}\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    \n",
    "    return llm.chat.completions.create(model='gemma:7b', messages=messages, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7179f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž --- DEBUGGING QUERY: 'What is the password to the prefects' bathroom?' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3dbb1e8e34479e95c20fd922074e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "ðŸ“š Found 10 docs via Vector Search.\n",
      "ðŸ¥‡ Top 5 Documents after Reranking:\n",
      "\n",
      "Rank 1 | Score: 0.0006\n",
      "Source: 4.pdf\n",
      "Content Snippet: Invisibility Cloak, looking around.\n",
      "His immediate reaction was that it would be worth becoming a prefect just to be able to use this \n",
      "bathroom.  It wa...\n",
      "--------------------------------------------------\n",
      "Rank 2 | Score: 0.0004\n",
      "Source: 2.pdf\n",
      "Content Snippet: CHAPTER  FIVE \n",
      "Â‘ 84 Â‘ \n",
      "They didnâ€™t know the new yearâ€™s password, not having met a \n",
      "Gryffindor prefect yet, but help came almost immediately; they \n",
      "hea...\n",
      "--------------------------------------------------\n",
      "Rank 3 | Score: 0.0003\n",
      "Source: 4.pdf\n",
      "Content Snippet: \"Password?\" she said as they approached.\n",
      "\"Balderdash,\" said George, \"a prefect downstairs told me.\"\n",
      " The portrait swung forward to reveal a hole in th...\n",
      "--------------------------------------------------\n",
      "Rank 4 | Score: 0.0001\n",
      "Source: 1.pdf\n",
      "Content Snippet: \"Password?\" she said. \"Caput Draconis,\" said Percy, and the portrait swung forward to reveal a\n",
      "round hole in the wall. They all scrambled through it -...\n",
      "--------------------------------------------------\n",
      "Rank 5 | Score: 0.0001\n",
      "Source: 5.pdf\n",
      "Content Snippet: room and had come to a halt in fron t of the portrait of the Fat Lady \n",
      "before he realized that he did not know the new password....\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ”Ž --- DEBUGGING QUERY: 'How do Horcruxes work?' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544a81abad5d47348988a1525b60d699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "ðŸ“š Found 10 docs via Vector Search.\n",
      "ðŸ¥‡ Top 5 Documents after Reranking:\n",
      "\n",
      "Rank 1 | Score: 0.7473\n",
      "Source: 6.pdf\n",
      "Content Snippet: HORCRUXES \n",
      "Â‘ 501 Â‘ \n",
      "â€œI still donâ€™t understand,â€ said Harry. \n",
      "â€œWell, it worked as a Horcrux is supposed to work â€” in other \n",
      "words, the fragment of soul...\n",
      "--------------------------------------------------\n",
      "Rank 2 | Score: 0.1315\n",
      "Source: 6.pdf\n",
      "Content Snippet: the term. A Horcrux is the word us ed for an object in which a per-\n",
      "son has concealed part  of their soul.â€ \n",
      "â€œI donâ€™t quite understand how th at works...\n",
      "--------------------------------------------------\n",
      "Rank 3 | Score: 0.0495\n",
      "Source: 6.pdf\n",
      "Content Snippet: about that precious fragment of his soul concealed within it. The \n",
      "point of a Horcrux is, as Professor Slughorn explained, to keep part \n",
      "of the self h...\n",
      "--------------------------------------------------\n",
      "Rank 4 | Score: 0.0009\n",
      "Source: 7.pdf\n",
      "Content Snippet: how to make a Horcrux but the time he asked Slughorn about them. I think\n",
      "youâ€™re right, Hermione, that could easily have been where he got the informa-...\n",
      "--------------------------------------------------\n",
      "Rank 5 | Score: 0.0005\n",
      "Source: 7.pdf\n",
      "Content Snippet: Hermione rummaged for a moment and then extracted from the pile a large\n",
      "volume, bound in faded black leather. She looked a little nauseated and held i...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def debug_retrieval(query, retriever, top_k=5):\n",
    "    print(f\"\\nðŸ”Ž --- DEBUGGING QUERY: '{query}' ---\")\n",
    "    \n",
    "    # 1. Get Initial Vector Search Results\n",
    "    initial_docs = retriever.retrive(query, top_k=top_k*2)\n",
    "    print(f\"ðŸ“š Found {len(initial_docs)} docs via Vector Search.\")\n",
    "    \n",
    "    # 2. Apply Reranking\n",
    "    reranked_docs = retriever.rerank(query, initial_docs, top_k=top_k)\n",
    "    \n",
    "    print(f\"ðŸ¥‡ Top {top_k} Documents after Reranking:\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        print(f\"Rank {i+1} | Score: {doc.get('rerank_score', 0):.4f}\")\n",
    "        print(f\"Source: {doc['metadata'].get('source_file', 'Unknown')}\")\n",
    "        print(f\"Content Snippet: {doc['content'][:150]}...\") # Show first 150 chars\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# --- TEST CASES ---\n",
    "# Specific Fact\n",
    "debug_retrieval(\"What is the password to the prefects' bathroom?\", rag_retriever)\n",
    "\n",
    "# Abstract Concept\n",
    "debug_retrieval(\"How do Horcruxes work?\", rag_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93dc734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Searching for: describe Harry's Sorting Hat experience\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adec28e2b7fa4bdcae9ac231eea7dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "ðŸ”„ Reranking results...\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"describe Harry's Sorting Hat experience\", rag_retriever, llm_client,use_rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "caf776bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-145', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Harry's Sorting Hat experience was fraught with initial fear and uncertainty, but ultimately ended with him being sorted into Gryffindor. He recalled feeling terrified while waiting for the test, but the Hat itself seemed to acknowledge his anxieties, remarking on the increased branchings in that year's students. Despite the initial tension, Harry felt calmer in Dumbledore's office and was able to share his dream with the headmaster. The Hat revealed that it considers individual choices when making its decisions, as evidenced by its sorting of Snape into Slytherin despite his preference for Gryffindor.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1763621841, model='gemma:7b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=115, prompt_tokens=492, total_tokens=607, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d0ae45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Harry's Sorting Hat experience was fraught with initial fear and uncertainty, but ultimately ended with him being sorted into Gryffindor. He recalled feeling terrified while waiting for the test, but the Hat itself seemed to acknowledge his anxieties, remarking on the increased branchings in that year's students. Despite the initial tension, Harry felt calmer in Dumbledore's office and was able to share his dream with the headmaster. The Hat revealed that it considers individual choices when making its decisions, as evidenced by its sorting of Snape into Slytherin despite his preference for Gryffindor.\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.choices[0].message.content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1bc16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Starting Evaluation on 4 questions...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fa39462b224ba99ec4a7193fef0f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "âŒ FAIL: Who is the Half-Blood Prince? (Expected keyword: 'Snape' not found)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38993bcf8d734b45a38dfcddb7060786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "âœ… PASS: What position does Harry play in Quidditch?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c03912c9da467abb9161729c9abfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "âœ… PASS: Who killed Dumbledore?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1af6c6a6ec4f4db0522432ccff49cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 documents (after filtering)\n",
      "âœ… PASS: What is the name of Hagrid's dog?\n",
      "\n",
      "ðŸ“Š FINAL SCORE: 3/4 (75.0%) Hit Rate\n"
     ]
    }
   ],
   "source": [
    "def evaluate_pipeline(test_dataset, retriever):\n",
    "    total = len(test_dataset)\n",
    "    hits = 0\n",
    "    \n",
    "    print(f\"ðŸ§ª Starting Evaluation on {total} questions...\\n\")\n",
    "    \n",
    "    for item in test_dataset:\n",
    "        query = item['question']\n",
    "        target_keyword = item['expected_keyword'] # A word that MUST be in the retrieved text\n",
    "        \n",
    "        # Get results\n",
    "        results = retriever.retrive(query, top_k=10)\n",
    "        reranked = retriever.rerank(query, results, top_k=5)\n",
    "        \n",
    "        # Check if any of the top 5 docs contain the keyword\n",
    "        found = False\n",
    "        for doc in reranked:\n",
    "            if target_keyword.lower() in doc['content'].lower():\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if found:\n",
    "            hits += 1\n",
    "            print(f\"âœ… PASS: {query}\")\n",
    "        else:\n",
    "            print(f\"âŒ FAIL: {query} (Expected keyword: '{target_keyword}' not found)\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š FINAL SCORE: {hits}/{total} ({(hits/total)*100:.1f}%) Hit Rate\")\n",
    "\n",
    "# Define a mini Golden Dataset\n",
    "test_data = [\n",
    "    {\"question\": \"Who is the Half-Blood Prince?\", \"expected_keyword\": \"Snape\"},\n",
    "    {\"question\": \"What position does Harry play in Quidditch?\", \"expected_keyword\": \"Seeker\"},\n",
    "    {\"question\": \"Who killed Dumbledore?\", \"expected_keyword\": \"Snape\"}, \n",
    "    {\"question\": \"What is the name of Hagrid's dog?\", \"expected_keyword\": \"Fang\"}\n",
    "]\n",
    "\n",
    "# Run the eval\n",
    "evaluate_pipeline(test_data, rag_retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
